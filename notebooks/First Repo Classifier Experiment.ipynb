{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "## Purpose \n",
    "\n",
    "Here we are experimenting with semi-supervised learning. We have prepared =~ 80 examples for our initial model. We will then use this model to classify more data, and then that model to classify more data, cyclically until we arrive at our goal of a good classifier.\n",
    "\n",
    "## Classifying Educational Projects\n",
    "\n",
    "The goal of this project is to classify github projects as primarily educational or not. This classification will then feed into a time series analysis of educational content on Github over time.\n",
    "\n",
    "## First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'archived': 0,\n",
       " 'forks': 1,\n",
       " 'has_downloads': 1,\n",
       " 'has_issues': 1,\n",
       " 'has_wiki': 1,\n",
       " 'is_edu': 0,\n",
       " 'is_fork': 0,\n",
       " 'network_count': 1,\n",
       " 'open_issues': 0,\n",
       " 'readme_words': ['htf', 'hack', 'future', 'net', 'challenge'],\n",
       " 'repo': 'Djohnnie/HTF2017',\n",
       " 'size': 79,\n",
       " 'stargazers': 1,\n",
       " 'subscribers': 2,\n",
       " 'watchers': 1}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, os, re\n",
    "import json, csv\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# =~ 80 examples done by hand... lets see if we can use semi-supervised learning to improve our training data!\n",
    "raw_training_data = []\n",
    "with jsonlines.open('../data/html/first_training_enriched_processed.jsonl') as reader:\n",
    "    raw_training_data = [record for record in reader]\n",
    "raw_training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nos average: 143.07, median: 77.0\n",
      "Yes average: 1003.18, median: 231.0\n",
      "README word count/education normalized cross correlation: 2.25\n"
     ]
    }
   ],
   "source": [
    "# Add the number of words, as it is a strong signal\n",
    "len_nos = [len(doc['readme_words']) for doc in raw_training_data if doc['is_edu'] == 0]\n",
    "len_yes = [len(doc['readme_words']) for doc in raw_training_data if doc['is_edu'] == 1]\n",
    "len_all = [len(doc['readme_words']) for doc in raw_training_data]\n",
    "max_all = max(len_all)\n",
    "normalized_all = [x/max_all for x in len_all]\n",
    "values = [doc['is_edu'] for doc in raw_training_data]\n",
    "\n",
    "print('Nos average: {:.2f}, median: {}'.format(np.average(len_nos), np.median(len_nos)))\n",
    "print('Yes average: {:.2f}, median: {}'.format(np.average(len_yes), np.median(len_yes)))\n",
    "\n",
    "print('README word count/education normalized cross correlation: {0:.2f}'.format(\n",
    "    np.correlate(values, normalized_all)[0]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'htf hack future net challenge'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "just_words = [' '.join(doc['readme_words']) for doc in raw_training_data]\n",
    "just_words[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "just_values = [doc['is_edu'] for doc in raw_training_data]\n",
    "just_values = np.array(just_values)\n",
    "just_values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rjurney/anaconda/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7,437 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_WORDS=1000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "tokenizer.fit_on_texts(just_words)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(just_words)\n",
    "word_index = tokenizer.word_index\n",
    "vocab_size = len(word_index)\n",
    "\n",
    "print('Found {:,} unique tokens.'.format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 1000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import preprocessing\n",
    "\n",
    "MAX_WORDS_PER_DOC=1000\n",
    "\n",
    "padded_sequences = preprocessing.sequence.pad_sequences(\n",
    "    sequences,\n",
    "    maxlen=MAX_WORDS_PER_DOC\n",
    ")\n",
    "padded_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>archived</th>\n",
       "      <th>forks</th>\n",
       "      <th>has_downloads</th>\n",
       "      <th>has_issues</th>\n",
       "      <th>has_wiki</th>\n",
       "      <th>is_fork</th>\n",
       "      <th>network_count</th>\n",
       "      <th>open_issues</th>\n",
       "      <th>size</th>\n",
       "      <th>stargazers</th>\n",
       "      <th>subscribers</th>\n",
       "      <th>watchers</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1378</td>\n",
       "      <td>0</td>\n",
       "      <td>2507</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>7</td>\n",
       "      <td>58529</td>\n",
       "      <td>133</td>\n",
       "      <td>26</td>\n",
       "      <td>133</td>\n",
       "      <td>73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>9740</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   archived  forks  has_downloads  has_issues  has_wiki  is_fork  \\\n",
       "0         0      1              1           1         1        0   \n",
       "1         0      1              1           0         1        1   \n",
       "2         0     94              1           1         1        0   \n",
       "3         0      2              1           1         1        0   \n",
       "4         0     29              1           1         1        0   \n",
       "5         0      1              1           1         1        0   \n",
       "\n",
       "   network_count  open_issues   size  stargazers  subscribers  watchers  \\\n",
       "0              1            0     79           1            2         1   \n",
       "1           1378            0   2507           0            1         0   \n",
       "2             94            7  58529         133           26       133   \n",
       "3              2            0     21           2            2         2   \n",
       "4             29            0   9740          25            2        25   \n",
       "5              1            0     24           3            2         3   \n",
       "\n",
       "   word_count  \n",
       "0           5  \n",
       "1         234  \n",
       "2          73  \n",
       "3          93  \n",
       "4           2  \n",
       "5           0  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare a numpy array containing our enrichment data...\n",
    "CONTINUOUS_KEYS = [\n",
    "    'forks',\n",
    "    'network_count',\n",
    "    'open_issues',\n",
    "    'size',\n",
    "    'stargazers',\n",
    "    'subscribers',\n",
    "    'watchers',\n",
    "    'word_count',\n",
    "]\n",
    "\n",
    "TOKEN_KEYS = [\n",
    "    'archived',\n",
    "    'has_downloads',\n",
    "    'has_issues',\n",
    "    'has_wiki',\n",
    "    'is_fork',\n",
    "]\n",
    "\n",
    "enrichment_data = []\n",
    "for doc in raw_training_data:\n",
    "    doc['word_count'] = len(doc['readme_words'])\n",
    "    new_doc = { your_key: doc[your_key] for your_key in CONTINUOUS_KEYS + TOKEN_KEYS }\n",
    "    enrichment_data.append(new_doc)\n",
    "\n",
    "df = pd.DataFrame(enrichment_data)\n",
    "df[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>archived</th>\n",
       "      <th>forks</th>\n",
       "      <th>has_downloads</th>\n",
       "      <th>has_issues</th>\n",
       "      <th>has_wiki</th>\n",
       "      <th>is_fork</th>\n",
       "      <th>network_count</th>\n",
       "      <th>open_issues</th>\n",
       "      <th>size</th>\n",
       "      <th>stargazers</th>\n",
       "      <th>subscribers</th>\n",
       "      <th>watchers</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.337913</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.392798</td>\n",
       "      <td>-0.385463</td>\n",
       "      <td>-0.486493</td>\n",
       "      <td>-0.342495</td>\n",
       "      <td>-0.368881</td>\n",
       "      <td>-0.342495</td>\n",
       "      <td>-0.280836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.337913</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.131313</td>\n",
       "      <td>-0.385463</td>\n",
       "      <td>-0.417227</td>\n",
       "      <td>-0.342589</td>\n",
       "      <td>-0.370256</td>\n",
       "      <td>-0.342589</td>\n",
       "      <td>-0.155115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.302495</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.357401</td>\n",
       "      <td>-0.136294</td>\n",
       "      <td>1.180953</td>\n",
       "      <td>-0.330136</td>\n",
       "      <td>-0.335887</td>\n",
       "      <td>-0.330136</td>\n",
       "      <td>-0.243504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.337532</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.392418</td>\n",
       "      <td>-0.385463</td>\n",
       "      <td>-0.488147</td>\n",
       "      <td>-0.342402</td>\n",
       "      <td>-0.368881</td>\n",
       "      <td>-0.342402</td>\n",
       "      <td>-0.232524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.327250</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.382141</td>\n",
       "      <td>-0.385463</td>\n",
       "      <td>-0.210886</td>\n",
       "      <td>-0.340248</td>\n",
       "      <td>-0.368881</td>\n",
       "      <td>-0.340248</td>\n",
       "      <td>-0.282483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.337913</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.392798</td>\n",
       "      <td>-0.385463</td>\n",
       "      <td>-0.488062</td>\n",
       "      <td>-0.342308</td>\n",
       "      <td>-0.368881</td>\n",
       "      <td>-0.342308</td>\n",
       "      <td>-0.283581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   archived     forks  has_downloads  has_issues  has_wiki  is_fork  \\\n",
       "0         0 -0.337913              1           1         1        0   \n",
       "1         0 -0.337913              1           0         1        1   \n",
       "2         0 -0.302495              1           1         1        0   \n",
       "3         0 -0.337532              1           1         1        0   \n",
       "4         0 -0.327250              1           1         1        0   \n",
       "5         0 -0.337913              1           1         1        0   \n",
       "\n",
       "   network_count  open_issues      size  stargazers  subscribers  watchers  \\\n",
       "0      -0.392798    -0.385463 -0.486493   -0.342495    -0.368881 -0.342495   \n",
       "1       0.131313    -0.385463 -0.417227   -0.342589    -0.370256 -0.342589   \n",
       "2      -0.357401    -0.136294  1.180953   -0.330136    -0.335887 -0.330136   \n",
       "3      -0.392418    -0.385463 -0.488147   -0.342402    -0.368881 -0.342402   \n",
       "4      -0.382141    -0.385463 -0.210886   -0.340248    -0.368881 -0.340248   \n",
       "5      -0.392798    -0.385463 -0.488062   -0.342308    -0.368881 -0.342308   \n",
       "\n",
       "   word_count  \n",
       "0   -0.280836  \n",
       "1   -0.155115  \n",
       "2   -0.243504  \n",
       "3   -0.232524  \n",
       "4   -0.282483  \n",
       "5   -0.283581  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# L2 normalize continuous variables\n",
    "df[CONTINUOUS_KEYS] = (df[CONTINUOUS_KEYS] - df[CONTINUOUS_KEYS].mean()) / df[CONTINUOUS_KEYS].std()\n",
    "df[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sort data\n",
    "indices = np.arange(padded_sequences.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "padded_sequences = padded_sequences[indices]\n",
    "df = df.iloc[indices]\n",
    "labels = just_values[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     data, \n",
    "#     labels, \n",
    "#     test_size=0.2,\n",
    "#     random_state=27\n",
    "# )\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "TRAINING_RUNS = 2\n",
    "\n",
    "seed = 11\n",
    "np.random.seed(seed)\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=TRAINING_RUNS, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Flatten, Dense, Embedding, Dropout, LSTM\n",
    "from keras.layers.merge import Concatenate\n",
    "from table import Table\n",
    "\n",
    "accuracies = []\n",
    "for i, train_test in enumerate(kfold.split(padded_sequences, labels)):\n",
    "    \n",
    "    train = train_test[0]\n",
    "    test = train_test[1]\n",
    "    \n",
    "    X_train = data[train]\n",
    "    X_test  = data[test]\n",
    "    y_train = labels[train]\n",
    "    y_test  = labels[test]\n",
    "    \n",
    "    main_model = Sequential()\n",
    "    \n",
    "    # Readme word embedding\n",
    "    readme_model = Sequential()\n",
    "    readme_model.add(\n",
    "        Embedding(\n",
    "            vocab_size, \n",
    "            64, \n",
    "            input_length=MAX_WORDS_PER_DOC\n",
    "        )\n",
    "    )\n",
    "    readme_model.add(Flatten())\n",
    "    readme_model.add(\n",
    "        Dense(32, activation='relu')\n",
    "    )\n",
    "\n",
    "    # Numeric Github API enrichment features\n",
    "    api_model = Sequential()\n",
    "    \n",
    "    \n",
    "    main_model.add(\n",
    "        Dense(1, activation='sigmoid')\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['acc', 'mse', 'mae', 'mape', 'cosine']\n",
    "    )\n",
    "    model.summary()\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        epochs=10,\n",
    "        batch_size=64,\n",
    "        validation_split=0.3,\n",
    "        verbose=0\n",
    "    )\n",
    "    scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    print(Table(model.metrics_names, [scores]))\n",
    "    \n",
    "    accuracy_pct = scores[1] * 100\n",
    "    accuracies.append(accuracy_pct)\n",
    "\n",
    "print(\n",
    "    Table(\n",
    "        ['Average','Median','Minimum','Maximum'],\n",
    "        [[np.average(accuracies), np.median(accuracies), np.max(accuracies), np.min(accuracies)]]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the First Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run me ONCE so we can go collect data from the (rate limited) Github API for our 1,000 record sample\n",
    "# with open('../data/html/documents.jsonl') as f:\n",
    "#     second_raw_training = []\n",
    "#     for line in f:\n",
    "#         record = json.loads(line)\n",
    "#         second_raw_training.append(record)\n",
    "\n",
    "# second_raw_training = random.sample(second_raw_training, 1000)\n",
    "\n",
    "# with open('../data/html/first_exploit_set.jsonl', 'w') as f:\n",
    "#     for record in second_raw_training:\n",
    "#         f.write( json.dumps(record) + '\\n' )\n",
    "\n",
    "# Run me every time thereafter you run this code block\n",
    "with open('../data/html/first_exploit_set.jsonl') as f:\n",
    "    second_raw_training = []\n",
    "    for line in f:\n",
    "        record = json.loads(line)\n",
    "        second_raw_training.append(record)\n",
    "len(second_raw_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_just_words = [' '.join(doc['readme_words']) for doc in second_raw_training]\n",
    "\n",
    "second_sequences = tokenizer.texts_to_sequences(second_just_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_truncated = preprocessing.sequence.pad_sequences(\n",
    "    second_sequences,\n",
    "    maxlen=MAX_WORDS_PER_DOC\n",
    ")\n",
    "second_truncated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_truncated_enriched ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_model_predictions = model.predict_classes(second_truncated)\n",
    "first_predictions_list = list(first_model_predictions.T[0])\n",
    "\n",
    "first_positive = [{**datum, **{'pred_edu': pred}} for datum, pred in zip(second_raw_training, first_predictions_list) if pred == 1]\n",
    "first_pos_repos = [(doc['pred_edu'], 'https://github.com/' + doc['repo']) for doc in first_positive]\n",
    "first_pos_repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
